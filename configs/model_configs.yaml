# Model Configuration File
# Each top-level key represents a model to train

xgboost:
  init_params:
    objective: "reg:squarederror"
    n_estimators: 500  # Starting point
    learning_rate: 0.05
    max_depth: 7
    subsample: 0.8
    colsample_bytree: 0.8
    gamma: 0.1
    reg_alpha: 0.5  # Add L1 regularization
    reg_lambda: 0.7  # Add L2 regularization
  tuning:
    method: "random"
    n_iter: 30
    cv: 5
  epochs: 20  # Will add 100 trees per epoch after first

lightgbm:
  init_params:
    objective: "regression"
    learning_rate: 0.05
    n_estimators: 1  # Start with 1 estimator
    num_leaves: 31
    metric: ["l2", "mae"]
    verbosity: -1
  epochs: 10

neural_network:
  init_params:
    hidden_size: 64
    learning_rate: 0.001
  epochs: 10
  batch_size: 32

svm:
  init_params:
    kernel: 'rbf'       # rbf, linear, poly, sigmoid
    C: 1.0              # Regularization parameter
    epsilon: 0.1        # Epsilon-tube for SVR
    gamma: 'scale'      # Kernel coefficient
  epochs: 1             # SVM trains fully in one epoch

pinn:
  init_params:
    hidden_size: 128
    learning_rate: 0.0001
    lambda_physics: 0.001  # Weight for physics loss term
  epochs: 100
  batch_size: 16