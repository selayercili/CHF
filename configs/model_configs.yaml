# Model Configuration File
# Each top-level key represents a model to train

xgboost:
  # Model initialization parameters
  init_params:
    n_estimators: 100
    max_depth: 6
    learning_rate: 0.3
    objective: "reg:squarederror"
    random_state: 42
  
  # Training parameters
  epochs: 10
  batch_size: 32
  learning_rate: 0.001  # For optimizer if applicable
  
  # Additional model-specific parameters
  early_stopping_rounds: 10
  eval_metric: "rmse"

# lightgbm:
#   init_params:
#     num_leaves: 31
#     learning_rate: 0.05
#     feature_fraction: 0.9
#     bagging_fraction: 0.8
#     bagging_freq: 5
#     verbose: 0
#     random_state: 42
  
#   epochs: 15
#   batch_size: 64
#   learning_rate: 0.001
  
#   # LightGBM specific
#   num_boost_round: 100
#   early_stopping_rounds: 10

neural_network:
  init_params:
    input_size: 9  # Number of features
    hidden_size: 64
    learning_rate: 0.001
  epochs: 50
  batch_size: 32
