# Model Configuration File
# Each top-level key represents a model to train

xgboost:
  init_params:
    objective: "reg:squarederror"
    # Hyperparameters to tune
    learning_rate: [0.01, 0.05, 0.1]  # Also called eta
    max_depth: [3, 5, 7]
    n_estimators: [100, 200, 500]
    subsample: [0.6, 0.8, 1.0]
    colsample_bytree: [0.6, 0.8, 1.0]
    gamma: [0, 0.1, 0.2]
  tuning:
    method: "random"  # Options: grid, random
    n_iter: 20        # For random search
    cv: 5             # Cross-validation folds
    scoring: "neg_mean_squared_error"
  epochs: 30        # More trees for better convergence

lightgbm:
  init_params:
    objective: "regression"
    learning_rate: 0.05
    n_estimators: 1  # Start with 1 estimator
    num_leaves: 31
    metric: ["l2", "mae"]
    verbosity: -1
  epochs: 10

neural_network:
  init_params:
    hidden_size: 64
    learning_rate: 0.001
  epochs: 10
  batch_size: 32

svm:
  init_params:
    kernel: 'rbf'       # rbf, linear, poly, sigmoid
    C: 1.0              # Regularization parameter
    epsilon: 0.1        # Epsilon-tube for SVR
    gamma: 'scale'      # Kernel coefficient
  epochs: 1             # SVM trains fully in one epoch

# Add this to your configs/model_configs.yaml file

pinn:
  init_params:
    hidden_size: 128           # Increased for more capacity
    learning_rate: 0.0005      # Reduced learning rate
    lambda_physics: 0.01       # Much smaller physics weight
    debug: true                # Enable debugging
    physics_warmup_epochs: 10  # Gradual physics introduction
  epochs: 100                  # More epochs with patience
  batch_size: 64              # Larger batch size for stability
  learning_rate: 0.0005       # Consistent with init_params