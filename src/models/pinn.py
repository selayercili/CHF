"""
Completely Fixed Physics-Informed Neural Network (PINN) Model Implementation

Key fixes:
1. Proper data scaling and normalization
2. Fixed physics equation with correct units and stability
3. Better network architecture and initialization
4. Improved loss balancing and training strategy
5. Fixed gradient flow and numerical stability
6. Proper feature handling and data preparation
"""
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from typing import Dict, Any, Optional, Tuple
from pathlib import Path
import pickle
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import warnings

class Pinn:
    """Fixed PINN model with proper physics integration and numerical stability."""
    
    def __init__(self, hidden_layers: list = None, learning_rate: float = 0.001, 
                 lambda_physics: float = 0.1, **kwargs):
        """
        Args:
            hidden_layers: List of hidden layer sizes [64, 64, 32]
            learning_rate: Optimizer learning rate
            lambda_physics: Weight for physics loss term
        """
        self.hidden_layers = hidden_layers or [64, 64, 32]
        self.learning_rate = learning_rate
        self.lambda_physics = lambda_physics
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.is_fitted = False
        self.epoch = 0
        self.input_size = None
        
        # Use StandardScaler for inputs and MinMaxScaler for target (better for physics)
        self.input_scaler = StandardScaler()
        self.target_scaler = MinMaxScaler(feature_range=(0.1, 2.0))  # Reasonable CHF range
        
        # Physics parameters - more conservative initialization
        self.debug = kwargs.get('debug', False)
        self.physics_warmup_epochs = kwargs.get('physics_warmup_epochs', 100)
        self.min_epochs_before_physics = 20  # Ensure data learning first
        
        # Better CHF equation parameters
        self.chf_param_bounds = {
            'A': (0.1, 10.0),
            'B': (1e-8, 1e-3),
            'C': (0.01, 5.0)
        }
        
        # Feature mapping (ensure consistency)
        self.expected_features = [
            'pressure_MPa', 'mass_flux_kg_m2_s', 'x_e_out__', 
            'D_h_mm', 'length_mm', 'geometry_encoded'
        ]
        
        # Training history
        self.loss_history = {'total': [], 'data': [], 'physics': [], 'epoch': []}

    def _prepare_data(self, data: pd.DataFrame, is_training: bool = True) -> Tuple[torch.Tensor, torch.Tensor]:
        """FIXED: Robust data preparation with consistent data types and feature ordering."""
        
        print(f"_prepare_data - Input shape: {data.shape}, Training: {is_training}")
        print(f"_prepare_data - Columns: {list(data.columns)}")
        
        # CRITICAL: Check and report data types
        print("Data types before processing:")
        for col in data.columns:
            print(f"  {col}: {data[col].dtype}")
        
        # Find target column
        target_candidates = ['chf_exp', 'CHF_exp', 'CHF', 'target']
        target_col = None
        for candidate in target_candidates:
            matching_cols = [col for col in data.columns if candidate.lower() in col.lower()]
            if matching_cols:
                target_col = matching_cols[0]
                break
        
        if target_col is None:
            # If no target found, assume last column is target
            target_col = data.columns[-1]
            warnings.warn(f"No standard target column found, using last column: {target_col}")
        
        print(f"Using target column: {target_col}")
        
        # Identify feature columns more robustly
        exclude_patterns = ['chf', 'target', 'cluster', 'label', 'id']
        feature_cols = []
        
        for col in data.columns:
            if col == target_col:
                continue
            if any(pattern in col.lower() for pattern in exclude_patterns):
                continue
            feature_cols.append(col)
        
        print(f"Feature columns ({len(feature_cols)}): {feature_cols}")
        
        # CRITICAL: Store feature column order for consistency
        self._last_feature_cols = feature_cols
        
        # Validate we have expected features
        if len(feature_cols) < 5:  # Minimum expected features
            raise ValueError(f"Too few features found: {len(feature_cols)}. Expected at least 5.")
        
        # CRITICAL FIX: Force consistent data types
        try:
            # Convert ALL columns to float64 first to ensure consistency
            data_copy = data.copy()
            
            # Convert feature columns to float64
            for col in feature_cols:
                if data_copy[col].dtype != np.float64:
                    print(f"Converting {col} from {data_copy[col].dtype} to float64")
                    data_copy[col] = data_copy[col].astype(np.float64)
            
            # Convert target column to float64  
            if data_copy[target_col].dtype != np.float64:
                print(f"Converting {target_col} from {data_copy[target_col].dtype} to float64")
                data_copy[target_col] = data_copy[target_col].astype(np.float64)
            
            # Extract data with consistent types
            X = data_copy[feature_cols].values.astype(np.float32)  # Final conversion to float32
            y = data_copy[target_col].values.astype(np.float32)
            
            print(f"After type conversion - X dtype: {X.dtype}, y dtype: {y.dtype}")
            print(f"Raw data - X shape: {X.shape}, y shape: {y.shape}")
            print(f"Raw data - X range: [{X.min():.6f}, {X.max():.6f}]")
            print(f"Raw data - y range: [{y.min():.6f}, {y.max():.6f}]")
            
            # Check for any remaining issues
            print("Feature-wise ranges:")
            for i, col in enumerate(feature_cols):
                print(f"  {col}: [{X[:, i].min():.6f}, {X[:, i].max():.6f}]")
            
        except Exception as e:
            raise ValueError(f"Error extracting data with type conversion: {e}")
        
        # Check for missing values
        if np.isnan(X).any() or np.isnan(y).any():
            print("Warning: Found NaN values in data")
            # Simple imputation - replace NaN with column mean
            X = np.nan_to_num(X, nan=np.nanmean(X, axis=0))
            y = np.nan_to_num(y, nan=np.nanmean(y))
        
        # Initialize or fit scalers
        if is_training and (not self.is_fitted or self.model is None):
            print("Fitting scalers with consistent data types...")
            X_scaled = self.input_scaler.fit_transform(X)
            y_scaled = self.target_scaler.fit_transform(y.reshape(-1, 1)).flatten()
            
            print(f"Input scaling - Original range: [{X.min():.6f}, {X.max():.6f}]")
            print(f"Input scaling - Scaled range: [{X_scaled.min():.6f}, {X_scaled.max():.6f}]")
            print(f"Target scaling - Original range: [{y.min():.6f}, {y.max():.6f}]")
            print(f"Target scaling - Scaled range: [{y_scaled.min():.6f}, {y_scaled.max():.6f}]")
            
            # Store scaler information for debugging
            print(f"Input scaler - mean: {self.input_scaler.mean_[:3]}...")
            print(f"Input scaler - scale: {self.input_scaler.scale_[:3]}...")
            print(f"Target scaler - data_min: {self.target_scaler.data_min_}")
            print(f"Target scaler - data_max: {self.target_scaler.data_max_}")
            
        else:
            print("Using existing scalers with type-corrected data...")
            X_scaled = self.input_scaler.transform(X)
            y_scaled = self.target_scaler.transform(y.reshape(-1, 1)).flatten()
            print(f"Using existing scalers - X_scaled range: [{X_scaled.min():.6f}, {X_scaled.max():.6f}]")
            print(f"Using existing scalers - y_scaled range: [{y_scaled.min():.6f}, {y_scaled.max():.6f}]")
        
        # Initialize model if needed
        if self.model is None:
            self.input_size = X_scaled.shape[1]
            self.model = self._build_model(self.input_size)
            self.optimizer = torch.optim.AdamW(
                self.model.parameters(), 
                lr=self.learning_rate, 
                weight_decay=1e-4
            )
            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer, mode='min', factor=0.5, patience=20, verbose=True
            )
            print(f"âœ“ Initialized PINN model with {self.input_size} input features")
        
        # Validate input size consistency
        if X_scaled.shape[1] != self.input_size:
            raise ValueError(f"Feature size mismatch! Expected {self.input_size}, got {X_scaled.shape[1]}. "
                           f"Current features: {feature_cols}")
        
        # Convert to tensors
        X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(self.device)
        y_tensor = torch.tensor(y_scaled, dtype=torch.float32).to(self.device)
        
        return X_tensor, y_tensor

    def _build_model(self, input_size: int) -> nn.Module:
        """Build improved neural network with learnable physics parameters."""
        
        layers = []
        prev_size = input_size
        
        # Build hidden layers
        for i, hidden_size in enumerate(self.hidden_layers):
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.BatchNorm1d(hidden_size))
            layers.append(nn.LeakyReLU(0.2))
            if i < len(self.hidden_layers) - 1:  # Don't add dropout to last hidden layer
                layers.append(nn.Dropout(0.15))
            prev_size = hidden_size
        
        # Output layer
        layers.append(nn.Linear(prev_size, 1))
        
        model = nn.Sequential(*layers).to(self.device)
        
        # Better weight initialization
        def init_weights(m):
            if isinstance(m, nn.Linear):
                torch.nn.init.xavier_normal_(m.weight)
                if m.bias is not None:
                    torch.nn.init.zeros_(m.bias)
        
        model.apply(init_weights)
        
        # Physics parameters with proper constraints
        physics_params = {}
        for name, (min_val, max_val) in self.chf_param_bounds.items():
            # Initialize in the middle of the range
            init_val = (min_val + max_val) / 2
            param = nn.Parameter(torch.tensor(init_val, device=self.device))
            model.register_parameter(f'physics_{name}', param)
            physics_params[name] = param
        
        # Store reference to physics parameters
        self.physics_params = physics_params
        
        print(f"Model architecture: {[input_size] + self.hidden_layers + [1]}")
        total_params = sum(p.numel() for p in model.parameters())
        print(f"Total parameters: {total_params}")
        
        return model

    def _get_physics_params(self) -> Dict[str, torch.Tensor]:
        """Get constrained physics parameters."""
        constrained_params = {}
        for name, param in self.physics_params.items():
            min_val, max_val = self.chf_param_bounds[name]
            # Use sigmoid to constrain parameters
            constrained_val = min_val + (max_val - min_val) * torch.sigmoid(param)
            constrained_params[name] = constrained_val
        return constrained_params

    def _convert_units_stable(self, inputs: torch.Tensor) -> Dict[str, torch.Tensor]:
        """FIXED: Stable unit conversion with proper bounds."""
        
        # Expecting: pressure_MPa, mass_flux_kg_m2_s, x_e_out__, D_h_mm, length_mm, geometry_encoded
        pressure_MPa = torch.clamp(inputs[:, 0], min=0.1, max=30.0)
        mass_flux = torch.clamp(inputs[:, 1], min=50.0, max=5000.0)
        quality = torch.clamp(inputs[:, 2], min=-0.8, max=1.2)  # Allow subcooled
        D_h_mm = torch.clamp(inputs[:, 3], min=0.5, max=50.0)
        length_mm = torch.clamp(inputs[:, 4], min=10.0, max=5000.0)
        geometry = inputs[:, 5] if inputs.shape[1] > 5 else torch.zeros_like(pressure_MPa)
        
        return {
            'pressure_MPa': pressure_MPa,
            'mass_flux': mass_flux,
            'quality': quality,
            'D_h_mm': D_h_mm,
            'length_mm': length_mm,
            'geometry': geometry
        }

    def _stable_latent_heat(self, pressure_MPa: torch.Tensor) -> torch.Tensor:
        """Stable approximation of latent heat [kJ/kg]."""
        # Polynomial fit to steam table data (0.1-20 MPa range)
        # h_fg decreases from ~2257 kJ/kg to near 0 at critical point
        
        P = torch.clamp(pressure_MPa, min=0.1, max=20.0)
        
        # Coefficients from polynomial fit to steam tables
        a0 = 2257.0    # Base latent heat at low pressure
        a1 = -156.2    # Linear term
        a2 = 8.45      # Quadratic term
        a3 = -0.285    # Cubic term
        
        h_fg = a0 + a1 * P + a2 * P**2 + a3 * P**3
        
        # Ensure positive and reasonable values
        h_fg = torch.clamp(h_fg, min=100.0, max=2500.0)
        
        return h_fg

    def _physics_loss_improved(self, inputs: torch.Tensor, predictions: torch.Tensor) -> torch.Tensor:
        """MUCH IMPROVED: Physics loss with better stability and correct scaling."""
        
        try:
            # Get physics parameters
            params = self._get_physics_params()
            A = params['A']
            B = params['B'] 
            C = params['C']
            
            # Convert units
            units = self._convert_units_stable(inputs)
            pressure_MPa = units['pressure_MPa']
            mass_flux = units['mass_flux']
            quality = units['quality']
            D_h_mm = units['D_h_mm']
            length_mm = units['length_mm']
            
            # Get latent heat in kJ/kg
            h_fg = self._stable_latent_heat(pressure_MPa)
            
            # Convert to consistent units for calculation
            D_h_m = D_h_mm / 1000.0  # mm to m
            length_m = length_mm / 1000.0  # mm to m
            
            # Your CHF correlation (keeping exact form but with better stability)
            numerator = A - B * quality * h_fg
            
            # Denominator with stability checks
            L_over_Dh = length_m / (D_h_m + 1e-8)  # Avoid division by zero
            mass_flux_term = mass_flux + 1e-3  # Avoid very small mass flux
            
            denominator = C + length_m - (4.0 * B * L_over_Dh) / mass_flux_term
            
            # Prevent denominator from being too small
            denominator = denominator + torch.sign(denominator) * 1e-4
            
            # Physics CHF prediction [MW/mÂ²]
            q_chf_physics = numerator / denominator
            
            # Clamp to reasonable CHF range
            q_chf_physics = torch.clamp(q_chf_physics, min=0.05, max=50.0)
            
            # CRITICAL: Convert model prediction back to physical units
            pred_physical = self._to_physical_units(predictions)
            
            # Physics loss - compare in physical units
            mse_physics = torch.mean((pred_physical - q_chf_physics) ** 2)
            
            # Additional constraints for stability
            constraints = 0.0
            
            # 1. Reasonable parameter values
            param_penalty = (
                torch.relu(A - 10.0) ** 2 +  # A shouldn't be too large
                torch.relu(-A + 0.1) ** 2 +  # A shouldn't be too small
                torch.relu(B - 1e-3) ** 2 +  # B upper bound
                torch.relu(-B + 1e-8) ** 2 + # B lower bound
                torch.relu(C - 5.0) ** 2 +   # C upper bound
                torch.relu(-C + 0.01) ** 2   # C lower bound
            )
            
            # 2. Physical reasonableness
            negative_chf_penalty = torch.mean(torch.relu(-q_chf_physics) ** 2)
            extreme_values_penalty = torch.mean(torch.relu(q_chf_physics - 100.0) ** 2)
            
            # 3. Denominator stability
            small_denom_penalty = torch.mean(torch.relu(0.001 - torch.abs(denominator)) ** 2)
            
            constraints = (0.01 * param_penalty + 
                         0.1 * negative_chf_penalty + 
                         0.05 * extreme_values_penalty +
                         0.1 * small_denom_penalty)
            
            total_physics_loss = mse_physics + constraints
            
            # Debug information
            if self.debug and torch.rand(1) < 0.01:  # Print occasionally
                print(f"Physics: A={A.item():.4f}, B={B.item():.2e}, C={C.item():.4f}")
                print(f"CHF range: [{q_chf_physics.min().item():.3f}, {q_chf_physics.max().item():.3f}]")
                print(f"Pred range: [{pred_physical.min().item():.3f}, {pred_physical.max().item():.3f}]")
                print(f"Physics loss: {mse_physics.item():.6f}, Constraints: {constraints.item():.6f}")
            
            return total_physics_loss
            
        except Exception as e:
            if self.debug:
                print(f"Physics loss failed: {e}")
            return torch.tensor(0.0, device=self.device, requires_grad=True)

    def _to_physical_units(self, predictions: torch.Tensor) -> torch.Tensor:
        """Convert scaled predictions back to physical units."""
        pred_np = predictions.detach().cpu().numpy().reshape(-1, 1)
        pred_physical_np = self.target_scaler.inverse_transform(pred_np).flatten()
        return torch.tensor(pred_physical_np, device=predictions.device, requires_grad=True)

    def train_epoch(self, train_data: pd.DataFrame, batch_size: int = 32, **kwargs) -> Dict[str, float]:
        """IMPROVED: Train one epoch with better loss balancing."""
        
        X_train, y_train = self._prepare_data(train_data, is_training=True)
        
        # Create dataset and dataloader
        dataset = torch.utils.data.TensorDataset(X_train, y_train)
        dataloader = torch.utils.data.DataLoader(
            dataset, batch_size=batch_size, shuffle=True, drop_last=False
        )
        
        self.model.train()
        
        total_loss = 0.0
        total_data_loss = 0.0
        total_physics_loss = 0.0
        n_batches = 0
        
        # Adaptive physics weight
        if self.epoch < self.min_epochs_before_physics:
            physics_weight = 0.0  # No physics initially
        elif self.epoch < self.physics_warmup_epochs:
            # Gradual ramp-up
            progress = (self.epoch - self.min_epochs_before_physics) / (self.physics_warmup_epochs - self.min_epochs_before_physics)
            physics_weight = self.lambda_physics * min(1.0, progress)
        else:
            physics_weight = self.lambda_physics
        
        for inputs, targets in dataloader:
            self.optimizer.zero_grad()
            
            # Forward pass
            outputs = self.model(inputs).squeeze()
            
            # Data loss (MSE)
            data_loss = nn.functional.mse_loss(outputs, targets)
            
            # Physics loss (only after warmup)
            if physics_weight > 0:
                physics_loss = self._physics_loss_improved(inputs, outputs)
            else:
                physics_loss = torch.tensor(0.0, device=self.device)
            
            # Combined loss
            total_batch_loss = data_loss + physics_weight * physics_loss
            
            # Check for numerical issues
            if not torch.isfinite(total_batch_loss):
                print(f"Warning: Non-finite loss at epoch {self.epoch}")
                continue
            
            # Backward pass
            total_batch_loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # Optimizer step
            self.optimizer.step()
            
            # Accumulate losses
            total_loss += total_batch_loss.item()
            total_data_loss += data_loss.item()
            total_physics_loss += physics_loss.item()
            n_batches += 1
        
        # Calculate averages
        avg_total_loss = total_loss / n_batches
        avg_data_loss = total_data_loss / n_batches
        avg_physics_loss = total_physics_loss / n_batches
        
        # Learning rate scheduling
        if self.scheduler:
            self.scheduler.step(avg_total_loss)
        
        # Update training history
        self.loss_history['epoch'].append(self.epoch)
        self.loss_history['total'].append(avg_total_loss)
        self.loss_history['data'].append(avg_data_loss)
        self.loss_history['physics'].append(avg_physics_loss)
        
        self.epoch += 1
        self.is_fitted = True
        
        # Create metrics
        metrics = {
            'loss': avg_total_loss,
            'rmse': np.sqrt(avg_data_loss),
            'data_loss': avg_data_loss,
            'physics_loss': avg_physics_loss,
            'physics_weight': physics_weight,
            'lr': self.optimizer.param_groups[0]['lr']
        }
        
        # Periodic logging
        if self.debug or self.epoch % 25 == 0:
            print(f"\nEpoch {self.epoch}:")
            print(f"  Total Loss: {avg_total_loss:.6f}")
            print(f"  Data Loss: {avg_data_loss:.6f}")
            print(f"  Physics Loss: {avg_physics_loss:.6f}")
            print(f"  Physics Weight: {physics_weight:.4f}")
            print(f"  Learning Rate: {self.optimizer.param_groups[0]['lr']:.2e}")
            
            # Show learned parameters
            params = self._get_physics_params()
            print(f"  Physics Params: A={params['A'].item():.4f}, "
                  f"B={params['B'].item():.2e}, C={params['C'].item():.4f}")
        
        return metrics

    def validate(self, val_data: pd.DataFrame) -> Dict[str, float]:
        """Enhanced validation with better metrics."""
        X_val, y_val = self._prepare_data(val_data, is_training=False)
        
        self.model.eval()
        with torch.no_grad():
            predictions = self.model(X_val).squeeze()
            
            # Basic losses
            mse = nn.functional.mse_loss(predictions, y_val)
            mae = nn.functional.l1_loss(predictions, y_val)
            
            # RÂ² calculation
            y_mean = y_val.mean()
            ss_tot = torch.sum((y_val - y_mean) ** 2)
            ss_res = torch.sum((y_val - predictions) ** 2)
            r2 = 1 - ss_res / ss_tot
            
            # Physical units comparison
            pred_physical = self._to_physical_units(predictions)
            y_physical = self._to_physical_units(y_val)
            
            mse_physical = nn.functional.mse_loss(pred_physical, y_physical)
            mae_physical = nn.functional.l1_loss(pred_physical, y_physical)
            
            # Maximum error
            max_error = torch.max(torch.abs(predictions - y_val))
            max_error_physical = torch.max(torch.abs(pred_physical - y_physical))
        
        return {
            'loss': mse.item(),
            'rmse': np.sqrt(mse.item()),
            'mae': mae.item(),
            'r2': r2.item(),
            'max_error': max_error.item(),
            'rmse_physical': np.sqrt(mse_physical.item()),
            'mae_physical': mae_physical.item(),
            'max_error_physical': max_error_physical.item()
        }

    def predict(self, data: pd.DataFrame) -> np.ndarray:
        """CRITICAL FIX: Make predictions with consistent data types and feature handling."""
        if self.model is None:
            raise ValueError("Model not trained yet")
        
        print(f"Predict input - Data shape: {data.shape}")
        print(f"Predict input - Columns: {list(data.columns)}")
        
        # CRITICAL: Check and report data types in prediction
        print("Data types in prediction:")
        for col in data.columns:
            print(f"  {col}: {data[col].dtype}")
        
        # CRITICAL: Use the EXACT SAME feature selection logic as training
        target_candidates = ['chf_exp', 'CHF_exp', 'CHF', 'target']
        target_col = None
        for candidate in target_candidates:
            matching_cols = [col for col in data.columns if candidate.lower() in col.lower()]
            if matching_cols:
                target_col = matching_cols[0]
                break
        
        # Identify feature columns EXACTLY like in training
        exclude_patterns = ['chf', 'target', 'cluster', 'label', 'id']
        feature_cols = []
        
        for col in data.columns:
            if target_col and col == target_col:
                continue
            if any(pattern in col.lower() for pattern in exclude_patterns):
                continue
            feature_cols.append(col)
        
        print(f"Predict - Using feature columns: {feature_cols}")
        print(f"Predict - Expected {self.input_size} features, got {len(feature_cols)}")
        
        if len(feature_cols) != self.input_size:
            raise ValueError(f"Feature mismatch! Expected {self.input_size} features, got {len(feature_cols)}. "
                           f"Training features: {self.input_size}, Test features: {len(feature_cols)}")
        
        try:
            # CRITICAL FIX: Force consistent data types like in training
            data_copy = data.copy()
            
            # Convert feature columns to float64 first, then float32
            for col in feature_cols:
                if data_copy[col].dtype != np.float64:
                    print(f"Converting {col} from {data_copy[col].dtype} to float64")
                    data_copy[col] = data_copy[col].astype(np.float64)
            
            X = data_copy[feature_cols].values.astype(np.float32)
            print(f"Predict - After type conversion - X dtype: {X.dtype}")
            print(f"Predict - Raw data range: [{X.min():.6f}, {X.max():.6f}]")
            
            # Feature-wise ranges for debugging
            print("Feature-wise ranges in prediction:")
            for i, col in enumerate(feature_cols):
                print(f"  {col}: [{X[:, i].min():.6f}, {X[:, i].max():.6f}]")
            
            # Check for missing values
            if np.isnan(X).any():
                print("Warning: Found NaN values in prediction data")
                X = np.nan_to_num(X, nan=np.nanmean(X, axis=0))
            
            X_scaled = self.input_scaler.transform(X)
            print(f"Predict - Scaled data range: [{X_scaled.min():.6f}, {X_scaled.max():.6f}]")
            
            # Check for scaling issues
            if np.abs(X_scaled).max() > 10:
                print(f"WARNING: Very large scaled values detected! Max: {np.abs(X_scaled).max()}")
                print("This suggests data distribution mismatch between training and testing")
            
            X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(self.device)
            
            self.model.eval()
            with torch.no_grad():
                predictions_scaled = self.model(X_tensor).squeeze().cpu().numpy()
            
            print(f"Predict - Model output range: [{predictions_scaled.min():.6f}, {predictions_scaled.max():.6f}]")
            
            # Convert back to physical units
            if predictions_scaled.ndim == 0:
                predictions_scaled = predictions_scaled.reshape(-1)
            
            predictions_scaled = predictions_scaled.reshape(-1, 1)
            predictions = self.target_scaler.inverse_transform(predictions_scaled).flatten()
            
            print(f"Predict - Final predictions range: [{predictions.min():.6f}, {predictions.max():.6f}]")
            
            return predictions
            
        except Exception as e:
            print(f"Prediction error: {e}")
            print(f"Available columns: {list(data.columns)}")
            print(f"Selected features: {feature_cols}")
            print("Data type info:")
            for col in feature_cols:
                if col in data.columns:
                    print(f"  {col}: {data[col].dtype}, range [{data[col].min()}, {data[col].max()}]")
            raise

    def get_physics_parameters(self) -> Dict[str, float]:
        """Get current physics parameters."""
        if self.model is None:
            return {}
        
        params = self._get_physics_params()
        return {name: param.item() for name, param in params.items()}

    def save(self, path: Path, metadata: Dict[str, Any] = None):
        """Save model with all necessary components and debugging info."""
        if self.model is None:
            raise ValueError("Model not initialized")
        
        # Get current feature info for debugging
        try:
            # Try to get feature column info from the last training
            current_features = getattr(self, '_last_feature_cols', None)
        except:
            current_features = None
        
        save_dict = {
            'model_state': self.model.state_dict(),
            'optimizer_state': self.optimizer.state_dict() if self.optimizer else None,
            'scheduler_state': self.scheduler.state_dict() if self.scheduler else None,
            'epoch': self.epoch,
            'input_size': self.input_size,
            'hidden_layers': self.hidden_layers,
            'lambda_physics': self.lambda_physics,
            'is_fitted': self.is_fitted,
            'input_scaler': self.input_scaler,
            'target_scaler': self.target_scaler,
            'loss_history': self.loss_history,
            'physics_param_bounds': self.chf_param_bounds,
            'expected_features': self.expected_features,
            'last_feature_cols': current_features,  # Save for debugging
            'scaler_feature_names': getattr(self.input_scaler, 'feature_names_in_', None)
        }
        
        if metadata:
            save_dict['metadata'] = metadata
        
        path = Path(path).with_suffix('.pth')
        torch.save(save_dict, path, pickle_protocol=pickle.HIGHEST_PROTOCOL)
        
        print(f"âœ“ Saved PINN model to {path}")
        print(f"  - Input size: {self.input_size}")
        print(f"  - Epoch: {self.epoch}")
        if current_features:
            print(f"  - Feature columns: {current_features}")
        
        # Save input scaler info separately for debugging
        scaler_info = {
            'mean_': self.input_scaler.mean_,
            'scale_': self.input_scaler.scale_,
            'var_': self.input_scaler.var_,
            'n_features_in_': self.input_scaler.n_features_in_
        }
        
        scaler_path = path.with_suffix('.scaler_info.pkl')
        with open(scaler_path, 'wb') as f:
            pickle.dump(scaler_info, f)
        print(f"âœ“ Saved scaler info to {scaler_path}")

    def load(self, path: Path):
        """Load model with proper restoration."""
        checkpoint = torch.load(path, map_location=self.device, weights_only=False)
        
        # Restore configuration
        self.input_size = checkpoint['input_size']
        self.hidden_layers = checkpoint['hidden_layers']
        self.lambda_physics = checkpoint['lambda_physics']
        self.epoch = checkpoint['epoch']
        self.is_fitted = checkpoint['is_fitted']
        
        # Restore scalers
        self.input_scaler = checkpoint['input_scaler']
        self.target_scaler = checkpoint['target_scaler']
        
        # Restore other attributes
        if 'loss_history' in checkpoint:
            self.loss_history = checkpoint['loss_history']
        if 'physics_param_bounds' in checkpoint:
            self.chf_param_bounds = checkpoint['physics_param_bounds']
        if 'expected_features' in checkpoint:
            self.expected_features = checkpoint['expected_features']
        
        # Rebuild model
        self.model = self._build_model(self.input_size)
        self.model.load_state_dict(checkpoint['model_state'])
        
        # Rebuild optimizer and scheduler
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(), 
            lr=self.learning_rate, 
            weight_decay=1e-4
        )
        if checkpoint.get('optimizer_state'):
            self.optimizer.load_state_dict(checkpoint['optimizer_state'])
        
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=20
        )
        if checkpoint.get('scheduler_state'):
            self.scheduler.load_state_dict(checkpoint['scheduler_state'])
        
        print(f"âœ“ Loaded PINN model from {path} (epoch {self.epoch})")
        return checkpoint.get('metadata', {})